{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037e4324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"documentos\": 50000,\n",
      "  \"features_tfidf\": 50000,\n",
      "  \"esparsidade\": 0.9977487532,\n",
      "  \"nao_nulos_na_matriz\": 5628117,\n",
      "  \"exemplo_termos\": [\n",
      "    \"00\",\n",
      "    \"000\",\n",
      "    \"000 00\",\n",
      "    \"000 000\",\n",
      "    \"000 years\",\n",
      "    \"007\",\n",
      "    \"01\",\n",
      "    \"02\",\n",
      "    \"05\",\n",
      "    \"06\",\n",
      "    \"07\",\n",
      "    \"08\",\n",
      "    \"10\",\n",
      "    \"10 000\",\n",
      "    \"10 10\",\n",
      "    \"10 15\",\n",
      "    \"10 20\",\n",
      "    \"10 Bethany\",\n",
      "    \"10 Dir\",\n",
      "    \"10 Fiend\"\n",
      "  ],\n",
      "  \"salvo_vectorizer\": \".\\\\tfidf_vectorizer.pkl\",\n",
      "  \"salvo_X\": \".\\\\X_tfidf.npz\",\n",
      "  \"salvo_y\": \".\\\\y_labels.npy\",\n",
      "  \"salvo_relatorio_vocab_csv\": \".\\\\relatorio_vocab_tfidf.csv\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TP5 — Tarefa 1: Criação de Features com TF-IDF (IMDB 50K Reviews)\n",
    "-----------------------------------------------------------------\n",
    "Rodar em **um único bloco** no Jupyter. Este código:\n",
    "1) Carrega o arquivo \"IMDB Dataset.csv\" (no mesmo diretório do notebook).\n",
    "2) Faz limpeza textual mínima (HTML/URLs/ruído).\n",
    "3) Vetoriza com TF-IDF (uni+bi-gramas) com bom controle de vocabulário.\n",
    "4) Salva artefatos para as próximas tarefas: vectorizer, X (matriz esparsa), y (rótulos) e um relatório CSV.\n",
    "5) Imprime um overview rápido (n docs, n features, esparsidade, etc.).\n",
    "\n",
    "Boas práticas: nomes claros em PT-BR, funções pequenas, configuração centralizada (Clean Code/SOLID).\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# Imports\n",
    "# ===============================\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# ===============================\n",
    "# 1) Configuração\n",
    "# ===============================\n",
    "ARQUIVO_PADRAO = \"IMDB Dataset.csv\"     # espera-se no mesmo diretório do notebook\n",
    "ALTERNATIVO = \"/mnt/data/IMDB Dataset.csv\"  # fallback (útil em alguns ambientes)\n",
    "DIRETORIO_SAIDA = \".\"                   # salvos no diretório atual\n",
    "\n",
    "cfg_tfidf: Dict[str, Any] = {\n",
    "    \"preprocessor\": None,                 # será injetado depois\n",
    "    \"token_pattern\": r\"(?u)\\b\\w\\w+\\b\",    # tokens com >=2 chars\n",
    "    \"stop_words\": \"english\",              # dataset está em inglês\n",
    "    \"ngram_range\": (1, 2),                # uni + bi-gramas\n",
    "    \"max_df\": 0.95,                       # remove termos muito frequentes\n",
    "    \"min_df\": 5,                          # remove termos muito raros\n",
    "    \"max_features\": 50000,                # controla vocabulário\n",
    "    \"strip_accents\": \"unicode\",\n",
    "    \"lowercase\": True,\n",
    "    \"norm\": \"l2\",\n",
    "    \"sublinear_tf\": True                  # tf = 1 + log(tf)\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# 2) Funções utilitárias\n",
    "# ===============================\n",
    "def caminho_csv() -> str:\n",
    "    \"\"\"Retorna o caminho existente para o CSV (prioriza o local).\"\"\"\n",
    "    if os.path.exists(ARQUIVO_PADRAO):\n",
    "        return ARQUIVO_PADRAO\n",
    "    if os.path.exists(ALTERNATIVO):\n",
    "        return ALTERNATIVO\n",
    "    raise FileNotFoundError(\n",
    "        f'Arquivo \"{ARQUIVO_PADRAO}\" não encontrado. '\n",
    "        f'Coloque-o no mesmo diretório do notebook ou ajuste o caminho.'\n",
    "    )\n",
    "\n",
    "def preprocessador_limpo(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpeza mínima e robusta:\n",
    "    - Remove HTML\n",
    "    - Remove URLs\n",
    "    - Normaliza aspas/backticks\n",
    "    - Remove pontuação extra/ruído e colapsa espaços\n",
    "    \"\"\"\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    texto = re.sub(r\"<.*?>\", \" \", texto)                       # HTML\n",
    "    texto = re.sub(r\"http\\S+|www\\.\\S+\", \" \", texto)            # URLs\n",
    "    texto = texto.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "    texto = texto.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    texto = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", texto)              # mantém letras, números e espaços\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
    "    return texto\n",
    "\n",
    "def validar_colunas(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Garante a presença de 'review' e 'sentiment' (case-insensitive).\"\"\"\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    esperado = {\"review\", \"sentiment\"}\n",
    "    faltantes = esperado.difference(df.columns)\n",
    "    if faltantes:\n",
    "        raise ValueError(f\"Colunas esperadas ausentes: {faltantes}. \"\n",
    "                         f\"Encontrado: {list(df.columns)}\")\n",
    "\n",
    "def mapear_rotulos(y_series: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Mapeia positive/negative -> 1/0; descarta valores inválidos depois.\"\"\"\n",
    "    mapa = {\"positive\": 1, \"negative\": 0}\n",
    "    y = y_series.str.lower().map(mapa)\n",
    "    return y.astype(\"float\")\n",
    "\n",
    "def calcular_esparsidade(matriz: csr_matrix) -> float:\n",
    "    \"\"\"Esparsidade = 1 - (nnz / total_de_elementos).\"\"\"\n",
    "    total = matriz.shape[0] * matriz.shape[1]\n",
    "    return 1.0 - (matriz.nnz / total)\n",
    "\n",
    "def montar_relatorio_vocab(vectorizer: TfidfVectorizer, n_docs: int, top_k: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tabela com termos, idf e DF estimado.\n",
    "    Fórmula sklearn: idf = log((n_samples + 1) / (df + 1)) + 1  =>  df ~= (n+1)/exp(idf-1) - 1\n",
    "    \"\"\"\n",
    "    vocab_inv = {idx: termo for termo, idx in vectorizer.vocabulary_.items()}\n",
    "    idfs = vectorizer.idf_\n",
    "    dfs_estimados = ((n_docs + 1) / np.exp(idfs - 1.0)) - 1.0\n",
    "    dfs_estimados = np.clip(dfs_estimados, 0, n_docs).astype(int)\n",
    "\n",
    "    df_vocab = pd.DataFrame({\n",
    "        \"indice\": np.arange(len(idfs), dtype=int),\n",
    "        \"termo\": [vocab_inv[i] for i in range(len(idfs))],\n",
    "        \"idf\": idfs,\n",
    "        \"df_estimado\": dfs_estimados\n",
    "    })\n",
    "\n",
    "    mais_comuns = df_vocab.sort_values([\"df_estimado\", \"termo\"], ascending=[False, True]).head(top_k).copy()\n",
    "    mais_raros  = df_vocab.sort_values([\"idf\", \"termo\"], ascending=[False, True]).head(top_k).copy()\n",
    "    mais_comuns[\"grupo\"] = \"Mais comuns (maior DF)\"\n",
    "    mais_raros[\"grupo\"]  = \"Mais raros (maior IDF)\"\n",
    "    relatorio = pd.concat([mais_comuns, mais_raros], axis=0, ignore_index=True)\n",
    "    return relatorio[[\"grupo\", \"termo\", \"df_estimado\", \"idf\"]]\n",
    "\n",
    "# ===============================\n",
    "# 3) Pipeline da Tarefa 1 (TF-IDF)\n",
    "# ===============================\n",
    "# 3.1 Carregar CSV\n",
    "caminho = caminho_csv()\n",
    "df = pd.read_csv(caminho)\n",
    "validar_colunas(df)\n",
    "\n",
    "# 3.2 Limpeza de nulos/espaços\n",
    "df[\"review\"] = df[\"review\"].fillna(\"\").astype(str)\n",
    "df = df[df[\"review\"].str.strip().ne(\"\")].copy()\n",
    "\n",
    "# 3.3 Mapear rótulos e filtrar inválidos\n",
    "y = mapear_rotulos(df[\"sentiment\"])\n",
    "mask_validos = ~y.isna()\n",
    "df = df.loc[mask_validos].reset_index(drop=True)\n",
    "y = y.loc[mask_validos].astype(int).to_numpy()\n",
    "\n",
    "# 3.4 Configurar e ajustar o TF-IDF\n",
    "cfg_tfidf_local = dict(cfg_tfidf)\n",
    "cfg_tfidf_local[\"preprocessor\"] = preprocessador_limpo\n",
    "vetorizador = TfidfVectorizer(**cfg_tfidf_local)\n",
    "\n",
    "X_tfidf: csr_matrix = vetorizador.fit_transform(df[\"review\"].tolist())\n",
    "n_docs, n_feats = X_tfidf.shape\n",
    "esparsidade = calcular_esparsidade(X_tfidf)\n",
    "\n",
    "# ===============================\n",
    "# 4) Persistência para próximas tarefas\n",
    "# ===============================\n",
    "os.makedirs(DIRETORIO_SAIDA, exist_ok=True)\n",
    "CAM_VETORIZADOR = os.path.join(DIRETORIO_SAIDA, \"tfidf_vectorizer.pkl\")\n",
    "CAM_X = os.path.join(DIRETORIO_SAIDA, \"X_tfidf.npz\")\n",
    "CAM_y = os.path.join(DIRETORIO_SAIDA, \"y_labels.npy\")\n",
    "CAM_RELATORIO = os.path.join(DIRETORIO_SAIDA, \"relatorio_vocab_tfidf.csv\")\n",
    "\n",
    "joblib.dump(vetorizador, CAM_VETORIZADOR)\n",
    "save_npz(CAM_X, X_tfidf)\n",
    "np.save(CAM_y, y)\n",
    "\n",
    "relatorio_vocab = montar_relatorio_vocab(vetorizador, n_docs=n_docs, top_k=25)\n",
    "relatorio_vocab.to_csv(CAM_RELATORIO, index=False)\n",
    "\n",
    "# ===============================\n",
    "# 5) Overview para conferência\n",
    "# ===============================\n",
    "overview = {\n",
    "    \"documentos\": int(n_docs),\n",
    "    \"features_tfidf\": int(n_feats),\n",
    "    \"esparsidade\": float(esparsidade),\n",
    "    \"nao_nulos_na_matriz\": int(X_tfidf.nnz),\n",
    "    \"exemplo_termos\": sorted(list(vetorizador.vocabulary_.keys()))[:20],\n",
    "    \"salvo_vectorizer\": CAM_VETORIZADOR,\n",
    "    \"salvo_X\": CAM_X,\n",
    "    \"salvo_y\": CAM_y,\n",
    "    \"salvo_relatorio_vocab_csv\": CAM_RELATORIO\n",
    "}\n",
    "\n",
    "print(json.dumps(overview, indent=2, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_libs_infnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
