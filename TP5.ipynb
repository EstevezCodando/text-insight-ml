{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037e4324",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LatentDirichletAllocation\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpora\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdictionary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dictionary\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcoherencemodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoherenceModel\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# =============================================\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# [SEÇÃO A] Configurações gerais\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# TP5 - Pipeline de NLP (Tarefas 1→7 em bloco único)\n",
    "# ——— Envio inicial: apenas TAREFA 1 (TF-IDF) ———\n",
    "# Observação: este bloco já está estruturado para\n",
    "# receber as Tarefas 2–7 sem duplicações.\n",
    "# =============================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# =========[SEÇÃO: IMPORTS ATUAIS — usados na Tarefa 1]=========\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# [SEÇÃO A] Configurações gerais\n",
    "\n",
    "SEMENTE_GLOBAL: int = 42\n",
    "np.random.seed(SEMENTE_GLOBAL)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConfigProjeto:\n",
    "    caminho_csv: str = \"IMDB Dataset.csv\"     # arquivo no mesmo diretório do .ipynb\n",
    "    coluna_texto: str = \"review\"\n",
    "    coluna_alvo: str = \"sentiment\"\n",
    "    salvar_dir: str = \"./artefatos\"           # pasta local para artefatos\n",
    "    # Hiperparâmetros do TF-IDF (pensados para classificação e t-SNE;\n",
    "    # LDA usará BoW específico na Tarefa 2)\n",
    "    max_features_tfidf: int = 50000\n",
    "    max_df: float = 0.9\n",
    "    min_df: int | float = 5\n",
    "    ngram_range: Tuple[int, int] = (1, 2)\n",
    "    norm: str = \"l2\"\n",
    "    stop_words: str | None = \"english\"\n",
    "    lowercase: bool = True\n",
    "\n",
    "CFG = ConfigProjeto()\n",
    "\n",
    "# =============================================\n",
    "# [SEÇÃO B] Pré-processamento textual\n",
    "# =============================================\n",
    "\n",
    "_regex_html = re.compile(r\"<.*?>\")\n",
    "_regex_url  = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "_regex_num  = re.compile(r\"\\d+\")\n",
    "_tbl_traducao_punct = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def limpar_texto(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpeza leve para IMDB:\n",
    "    - remove tags HTML, URLs, números, pontuação\n",
    "    - normaliza espaços\n",
    "    Evita stem/lemma nesta etapa para não atrapalhar análises futuras (ex.: LDA).\n",
    "    \"\"\"\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    t = texto\n",
    "    t = _regex_html.sub(\" \", t)\n",
    "    t = _regex_url.sub(\" \", t)\n",
    "    t = _regex_num.sub(\" \", t)\n",
    "    t = t.translate(_tbl_traducao_punct)\n",
    "    t = re.sub(r\"\\s+\", \" \", t, flags=re.MULTILINE).strip()\n",
    "    return t\n",
    "\n",
    "# =============================================\n",
    "# [SEÇÃO C] Carregamento da base\n",
    "# =============================================\n",
    "\n",
    "def carregar_base(caminho: str, coluna_texto: str, coluna_alvo: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(caminho):\n",
    "        raise FileNotFoundError(f\"Base não encontrada em: {caminho}\")\n",
    "    df = pd.read_csv(caminho)\n",
    "    colunas_necessarias = {coluna_texto, coluna_alvo}\n",
    "    ausentes = colunas_necessarias - set(df.columns)\n",
    "    if ausentes:\n",
    "        raise ValueError(f\"Colunas ausentes no CSV: {ausentes}\")\n",
    "    return df[[coluna_texto, coluna_alvo]].copy()\n",
    "\n",
    "# =============================================\n",
    "# [SEÇÃO D] Criação das features — TF-IDF (Tarefa 1)\n",
    "# =============================================\n",
    "\n",
    "def criar_tfidf(textos: List[str], cfg: ConfigProjeto) -> tuple[TfidfVectorizer, Any]:\n",
    "    \"\"\"\n",
    "    Ajusta TfidfVectorizer sobre o corpus.\n",
    "    Retorna (vetorizador, matriz_esparsa_TF-IDF).\n",
    "    \"\"\"\n",
    "    vetorizador = TfidfVectorizer(\n",
    "        lowercase=cfg.lowercase,\n",
    "        stop_words=cfg.stop_words,\n",
    "        max_df=cfg.max_df,\n",
    "        min_df=cfg.min_df,\n",
    "        ngram_range=cfg.ngram_range,\n",
    "        max_features=cfg.max_features_tfidf,\n",
    "        norm=cfg.norm\n",
    "    )\n",
    "    matriz = vetorizador.fit_transform(textos)\n",
    "    return vetorizador, matriz\n",
    "\n",
    "def salvar_artefatos_tfidf(vetorizador: TfidfVectorizer, X_esparsa, cfg: ConfigProjeto) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Salva:\n",
    "      - Vetorizador (PKL)\n",
    "      - Vocabulário (CSV)\n",
    "      - Metadados da matriz (JSON: shape, nnz, densidade)\n",
    "    Evita salvar a matriz completa (IO pesado).\n",
    "    \"\"\"\n",
    "    os.makedirs(cfg.salvar_dir, exist_ok=True)\n",
    "    caminhos: Dict[str, str] = {}\n",
    "\n",
    "    # Vetorizador\n",
    "    caminho_vec = os.path.join(cfg.salvar_dir, \"vetorizador_tfidf.pkl\")\n",
    "    with open(caminho_vec, \"wb\") as f:\n",
    "        pickle.dump(vetorizador, f)\n",
    "    caminhos[\"vetorizador\"] = caminho_vec\n",
    "\n",
    "    # Vocabulário (ordenado por índice interno)\n",
    "    vocab = pd.DataFrame({\n",
    "        \"termo\": list(vetorizador.vocabulary_.keys()),\n",
    "        \"indice\": list(vetorizador.vocabulary_.values())\n",
    "    }).sort_values(\"indice\").reset_index(drop=True)\n",
    "    caminho_vocab = os.path.join(cfg.salvar_dir, \"vocabulario_tfidf.csv\")\n",
    "    vocab.to_csv(caminho_vocab, index=False, encoding=\"utf-8\")\n",
    "    caminhos[\"vocabulario\"] = caminho_vocab\n",
    "\n",
    "    # Metadados da matriz\n",
    "    meta = {\n",
    "        \"shape\": list(X_esparsa.shape),\n",
    "        \"nnz\": int(X_esparsa.nnz),\n",
    "        \"densidade\": float(X_esparsa.nnz / (X_esparsa.shape[0] * X_esparsa.shape[1]))\n",
    "    }\n",
    "    caminho_meta = os.path.join(cfg.salvar_dir, \"tfidf_meta.json\")\n",
    "    with open(caminho_meta, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    caminhos[\"metadados\"] = caminho_meta\n",
    "\n",
    "    return caminhos\n",
    "\n",
    "# =============================================\n",
    "# [SEÇÃO E] Execução — Tarefa 1\n",
    "# =============================================\n",
    "\n",
    "# 1) Carrega base\n",
    "df_raw = carregar_base(CFG.caminho_csv, CFG.coluna_texto, CFG.coluna_alvo)\n",
    "\n",
    "# 2) Limpa textos\n",
    "df = df_raw.copy()\n",
    "df[\"texto_limpo\"] = df[CFG.coluna_texto].apply(limpar_texto)\n",
    "\n",
    "# 3) Cria TF-IDF (fit no corpus completo)\n",
    "vetorizador_tfidf, X_tfidf = criar_tfidf(df[\"texto_limpo\"].tolist(), CFG)\n",
    "\n",
    "# 4) (Opcional) Codifica rótulos para uso futuro em classificadores\n",
    "y_alvo = df[CFG.coluna_alvo].astype(\"category\").cat.codes  # negativo/positivo -> 0/1\n",
    "\n",
    "# 5) Salva artefatos para reuso nas Tarefas 2–7\n",
    "caminhos = salvar_artefatos_tfidf(vetorizador_tfidf, X_tfidf, CFG)\n",
    "\n",
    "# 6) Relatórios rápidos\n",
    "num_docs, num_feats = X_tfidf.shape\n",
    "densidade = X_tfidf.nnz / (num_docs * num_feats)\n",
    "\n",
    "print(\"=== [Tarefa 1] TF-IDF concluída ===\")\n",
    "print(f\"Documentos: {num_docs:,}\")\n",
    "print(f\"Features (vocabulário): {num_feats:,}\")\n",
    "print(f\"Densidade média da matriz: {densidade:.6f}\")\n",
    "print(f\"Classes codificadas encontradas: {sorted(set(y_alvo.tolist()))} -> 0=negativo, 1=positivo\")\n",
    "print(\"\\nArquivos salvos:\")\n",
    "for k, v in caminhos.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "# - Tarefa 2 (LDA): usaremos CountVectorizer/BoW específico para tópicos, reaproveitando df['texto_limpo'].\n",
    "\n",
    "# =============================================\n",
    "# [SEÇÃO F] Tarefa 2 — Modelagem de Tópicos com LDA\n",
    "# (Sequencial ao código da Tarefa 1, MESMO BLOCO)\n",
    "# - Usa df['texto_limpo'] já criado.\n",
    "# - Cria representação BoW (CountVectorizer) própria para LDA.\n",
    "# - Seleciona o nº de tópicos via métrica de coerência (c_v).\n",
    "# - Salva artefatos para as próximas tarefas.\n",
    "# =============================================\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Configurações específicas da Tarefa 2\n",
    "# ---------------------------------------------\n",
    "class ConfigLDA:\n",
    "    # candidatos de nº de tópicos; ajuste conforme necessidade/tempo\n",
    "    candidatos_k = [10, 15, 20]\n",
    "    max_iter = 10\n",
    "    learning_method = \"batch\"   # determinístico; \"online\" é mais rápido\n",
    "    learning_decay = 0.7\n",
    "    learning_offset = 10.0\n",
    "    max_features_bow = 50000\n",
    "    min_df = 5\n",
    "    max_df = 0.9\n",
    "    n_top_palavras = 12         # nº de termos por tópico no print/artefatos\n",
    "\n",
    "CFG_LDA = ConfigLDA()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Tokenização simples (compatível com a limpeza já feita)\n",
    "# ---------------------------------------------\n",
    "def tokenize(texto: str) -> list[str]:\n",
    "    # df['texto_limpo'] já removeu pontuação, números, links e normalizou espaços\n",
    "    return texto.lower().split()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Cria representação BoW específica para LDA\n",
    "# ---------------------------------------------\n",
    "vectorizador_bow = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",                 # corpus é EN\n",
    "    max_df=CFG_LDA.max_df,\n",
    "    min_df=CFG_LDA.min_df,\n",
    "    max_features=CFG_LDA.max_features_bow\n",
    ")\n",
    "X_bow = vectorizador_bow.fit_transform(df[\"texto_limpo\"].tolist())\n",
    "\n",
    "\n",
    "tokens_corpus = [tokenize(t) for t in df[\"texto_limpo\"].tolist()]\n",
    "dicionario = Dictionary(tokens_corpus)\n",
    "# Para coerência c_v com modelos externos (sklearn), passamos textos e dicionário; corpus não é obrigatório.\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Função utilitária: extrair tópicos (palavras) de um LDA sklearn\n",
    "# ---------------------------------------------\n",
    "def extrair_top_palavras(modelo_lda: LatentDirichletAllocation, vectorizer: CountVectorizer, n_top: int) -> list[list[str]]:\n",
    "    termos = vectorizer.get_feature_names_out()\n",
    "    topicos = []\n",
    "    for k in range(modelo_lda.components_.shape[0]):\n",
    "        pesos = modelo_lda.components_[k]\n",
    "        idx = pesos.argsort()[-n_top:][::-1]\n",
    "        topicos.append([termos[i] for i in idx])\n",
    "    return topicos\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Seleção de K por coerência (c_v)\n",
    "# ---------------------------------------------\n",
    "resultados_coerencia = []\n",
    "melhor_k = None\n",
    "melhor_score = -np.inf\n",
    "melhor_modelo = None\n",
    "\n",
    "for k in CFG_LDA.candidatos_k:\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=k,\n",
    "        max_iter=CFG_LDA.max_iter,\n",
    "        learning_method=CFG_LDA.learning_method,\n",
    "        learning_decay=CFG_LDA.learning_decay,\n",
    "        learning_offset=CFG_LDA.learning_offset,\n",
    "        random_state=SEMENTE_GLOBAL,\n",
    "        evaluate_every=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    lda.fit(X_bow)\n",
    "\n",
    "    # Obtém top palavras por tópico\n",
    "    top_palavras = extrair_top_palavras(lda, vectorizador_bow, CFG_LDA.n_top_palavras)\n",
    "\n",
    "    # Coerência c_v (usa ngrams, indireta; robusta para tópicos interpretáveis)\n",
    "    cm = CoherenceModel(\n",
    "        topics=top_palavras,\n",
    "        texts=tokens_corpus,\n",
    "        dictionary=dicionario,\n",
    "        coherence=\"c_v\"\n",
    "    )\n",
    "    score_cv = cm.get_coherence()\n",
    "    resultados_coerencia.append({\"k\": k, \"coerencia_c_v\": float(score_cv)})\n",
    "\n",
    "    if score_cv > melhor_score:\n",
    "        melhor_score = score_cv\n",
    "        melhor_k = k\n",
    "        melhor_modelo = lda\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Relatório e salvamento de artefatos\n",
    "# ---------------------------------------------\n",
    "print(\"=== [Tarefa 2] LDA — Seleção de Tópicos por Coerência (c_v) ===\")\n",
    "for r in resultados_coerencia:\n",
    "    print(f\"K={r['k']:>2}  |  Coerência (c_v) = {r['coerencia_c_v']:.4f}\")\n",
    "print(f\"\\nMelhor K selecionado: {melhor_k}  (c_v={melhor_score:.4f})\")\n",
    "\n",
    "topicos_melhor = extrair_top_palavras(melhor_modelo, vectorizador_bow, CFG_LDA.n_top_palavras)\n",
    "print(\"\\nTop palavras por tópico (melhor K):\")\n",
    "for i, palavras in enumerate(topicos_melhor, 1):\n",
    "    print(f\" - Tópico {i:02d}: {', '.join(palavras)}\")\n",
    "\n",
    "os.makedirs(CFG.salvar_dir, exist_ok=True)\n",
    "\n",
    "caminho_vec_bow = os.path.join(CFG.salvar_dir, \"vectorizador_bow.pkl\")\n",
    "with open(caminho_vec_bow, \"wb\") as f:\n",
    "    pickle.dump(vectorizador_bow, f)\n",
    "\n",
    "caminho_lda = os.path.join(CFG.salvar_dir, \"lda_model.pkl\")\n",
    "with open(caminho_lda, \"wb\") as f:\n",
    "    pickle.dump(melhor_modelo, f)\n",
    "\n",
    "# Tópicos e coerências\n",
    "caminho_topicos = os.path.join(CFG.salvar_dir, \"lda_topicos.csv\")\n",
    "pd.DataFrame(\n",
    "    {\"topico\": np.arange(1, len(topicos_melhor) + 1),\n",
    "     \"palavras\": [\", \".join(p) for p in topicos_melhor]}\n",
    ").to_csv(caminho_topicos, index=False, encoding=\"utf-8\")\n",
    "\n",
    "caminho_coerencia = os.path.join(CFG.salvar_dir, \"lda_coerencia.json\")\n",
    "with open(caminho_coerencia, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\"resultados\": resultados_coerencia, \"melhor_k\": melhor_k, \"coerencia_c_v\": melhor_score},\n",
    "        f, ensure_ascii=False, indent=2\n",
    "    )\n",
    "\n",
    "print(\"\\nArquivos salvos (Tarefa 2):\")\n",
    "print(f\"- vectorizador_bow.pkl: {caminho_vec_bow}\")\n",
    "print(f\"- lda_model.pkl:        {caminho_lda}\")\n",
    "print(f\"- lda_topicos.csv:      {caminho_topicos}\")\n",
    "print(f\"- lda_coerencia.json:   {caminho_coerencia}\")\n",
    "\n",
    "\n",
    "\n",
    "# - Tarefa 3–4 (Classificação/Avaliação): X_tfidf e y_alvo já estão prontos.\n",
    "# - Tarefa 5 (t-SNE): projete amostras de X_tfidf para 2D (cuidado com custo).\n",
    "# - Tarefa 6 (LIME/SHAP/force-plot): usar o melhor classificador ajustado sobre X_tfidf.\n",
    "# - Tarefa 7 (Conclusões): sumarize achados de tópicos e desempenho do classificador.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_libs_infnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
